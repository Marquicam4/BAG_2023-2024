{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6b2298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c42a926a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    when modi promised “minimum government maximum...\n",
       "1    talk all the nonsense and continue all the dra...\n",
       "2    what did just say vote for modi  welcome bjp t...\n",
       "3    asking his supporters prefix chowkidar their n...\n",
       "4    answer who among these the most powerful world...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Twitter_Data.csv')\n",
    "data2 = data[\"clean_text\"]\n",
    "data2[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf28a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 3 manzanas,  está  comiendo un  platano, maria \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "cadena = \"Hay 3 manzanas, @miguel está ⚡ comiendo un #gran platano, maria https://www.youtube.com/\"\n",
    "\n",
    "nueva_cadena = re.sub(r'https?://\\S+|www\\.\\S+|[@#]\\S*|[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]+','', cadena)\n",
    "\n",
    "print(nueva_cadena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a6f1ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola me gustan los  y los  \n"
     ]
    }
   ],
   "source": [
    "#cadena = \"hola me gustan los ⚡ y los ✌️ \"\n",
    "#nueva_cadena2 = re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]+', '', cadena)\n",
    "\n",
    "#print(nueva_cadena2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8645ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ejercicio 2: Limpieza de texto.\n",
    "import re\n",
    "def limpiar_texto(datos):\n",
    "    nueva_data=[]\n",
    "    for i in range(0,len(datos)):\n",
    "        nueva_cadena = re.sub(r'https?://\\S+|www\\.\\S+|[@#]\\S*|[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]+', '', data[i])\n",
    "        cadena_minuscula = nueva_cadena.lower()\n",
    "        tokenizer = TweetTokenizer()\n",
    "        separacion_data = tokenizer.tokenize(cadena_minuscula)\n",
    "        \n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        lista_vacia = []\n",
    "        for palabra in separacion_data:\n",
    "            if palabra not in stop_words:\n",
    "                lista_vacia.append((stemmer.stem(palabra)))\n",
    "        data_limpia = ' '.join(lista_vacia) #Entre los dos '' hay un espacio que significa que mis palabras en la cadena están separadas por un espacio.\n",
    "    \n",
    "    print(data_limpia)    \n",
    "    data_limpia[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00fb685",
   "metadata": {},
   "outputs": [],
   "source": [
    "limpiar_texto(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18253952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', ',', 'maría', ',']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "#Eliminar las palabras poco comunes e informativas.\n",
    "\n",
    "nueva_cadena = \"HOLA, MARÍA, doing\"\n",
    "cadena_minuscula = nueva_cadena.lower()\n",
    "tokenizer = TweetTokenizer()\n",
    "separacion_data = tokenizer.tokenize(cadena_minuscula)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lista_vacia = []\n",
    "for palabra in separacion_data:\n",
    "    if palabra not in stop_words:\n",
    "        lista_vacia.append(palabra)\n",
    "print(lista_vacia)\n",
    "\n",
    "        \n",
    "#print(separacion_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c76aee85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sultanpur uttar pradesh loksabha candid select pawan kumar pandey actual public want given vote modi current condid popular district candid bsp candid sonbhadra singh\n"
     ]
    }
   ],
   "source": [
    "#Este es añadida la parte de stemming.\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "nueva_cadena = \"sultanpur uttar pradesh loksabha candidate select pawan kumar pandey actually public want given vote modi but your current condidate not popular district your candidate bsp candidate sonbhadra singh\"\n",
    "cadena_minuscula = nueva_cadena.lower()\n",
    "tokenizer = TweetTokenizer()\n",
    "separacion_data = tokenizer.tokenize(cadena_minuscula)\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lista_vacia = []\n",
    "for palabra in separacion_data:\n",
    "    if palabra not in stop_words:\n",
    "        lista_vacia.append((stemmer.stem(palabra)))\n",
    "\n",
    "cadena_final = ' '.join(lista_vacia) #Entre los dos '' hay un espacio que significa que mis palabras en la cadena están separadas por un espacio.\n",
    "    \n",
    "print(cadena_final)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44402ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['modi', 'promised', '“minimum', 'government', 'maximum', 'governance”', 'expected', 'begin', 'difficult', 'job', 'reforming', 'state', 'take', 'years', 'get', 'justice', 'state', 'business', 'exit', 'psus', 'temples']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "texto = \"talk all the nonsense and continue all the drama will vote for modi\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lista_vacia = []\n",
    "for palabra in texto.split(' '):\n",
    "    if palabra not in stop_words:\n",
    "        lista_vacia.append(palabra)\n",
    "print(lista_vacia)\n",
    "\n",
    "#con split(' '), con el condicional if indicamos que solo se quede con las palabras que no se encuentren en \"stopwords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba88b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ejercicio 3\n",
    "from textblob import TextBlob\n",
    "\n",
    "def clasificador(data_limpia):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2eac7695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Texto': 'talk nonsens continu drama vote modi', 'Polaridad': 0.0, 'Clasificación': 'Neutro'}, {'Texto': 'sultanpur uttar pradesh loksabha candid select pawan kumar pandey actual public want given vote modi current condid popular district candid bsp candid sonbhadra singh', 'Polaridad': 0.34285714285714286, 'Clasificación': 'Feliz'}, {'Texto': 'one vote make differ anil kapoor answer modi elect 2019 clarion call extend support vote kar campaign', 'Polaridad': 0.8, 'Clasificación': 'Muy Feliz'}, {'Texto': 'modi promis minimum govern maximum govern expect begin difficult job reform state take year get justic state busi exit psus templ', 'Polaridad': -0.5, 'Clasificación': 'Muy Feliz'}, {'Texto': 'upcom elect india saga go import pair look current modi lead govt elect deal brexit combin week look juici bear imho', 'Polaridad': 0.5333333333333333, 'Clasificación': 'Muy Feliz'}]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import csv\n",
    "\n",
    "lista_de_textos = [\"talk nonsens continu drama vote modi\",\n",
    "                   \"sultanpur uttar pradesh loksabha candid select pawan kumar pandey actual public want given vote modi current condid popular district candid bsp candid sonbhadra singh\",\n",
    "                   \"one vote make differ anil kapoor answer modi elect 2019 clarion call extend support vote kar campaign\",\n",
    "                   \"modi promis minimum govern maximum govern expect begin difficult job reform state take year get justic state busi exit psus templ\",\n",
    "                    \"upcom elect india saga go import pair look current modi lead govt elect deal brexit combin week look juici bear imho\"]\n",
    "\n",
    "clasificacion_sentimiento = ['Muy feliz', 'Feliz', 'Contento', 'Molesto', 'Hater']\n",
    "resultados = []\n",
    "\n",
    "for texto in lista_de_textos:\n",
    "    blob = TextBlob(texto)\n",
    "    sentimiento = blob.sentiment #De cada texto obtenemos la polaridad (intervalo de -1 a 1) y la subjetividad (de 0 a 1)\n",
    "    polaridad = blob.sentiment.polarity #Nos interesa la polaridad.\n",
    "    \n",
    "    #print(sentimiento)\n",
    "    #print(polaridad)\n",
    "#Ahora según la polaridad, clasificamos los textos en distintas categorías. Usamos rangos de 0.5 en 0.5:\n",
    "\n",
    "    if polaridad > 0.5:\n",
    "        clasificacion_sentimiento = 'Muy Feliz'\n",
    "    elif polaridad > 0:\n",
    "        clasificacion_sentimiento = 'Feliz'\n",
    "    elif polaridad == 0:\n",
    "        clasificacion_sentimiento = 'Neutro'\n",
    "    elif polaridad >= -0.5:\n",
    "        clasificacion_polaridad = 'Molesto'\n",
    "    else:\n",
    "        clasificacion_sentimiento = 'Hater'\n",
    "    \n",
    "    resultados.append({\n",
    "        \"Texto\" : texto,\n",
    "        \"Polaridad\" : polaridad,\n",
    "        \"Clasificación\" : clasificacion_sentimiento\n",
    "    })\n",
    "    \n",
    "print(resultados)\n",
    "\n",
    "#Creo un csv para guardar los datos:\n",
    "\n",
    "with open(\"resultados_sentimientos.csv\", \"w\", newline = '') as csvfile:\n",
    "    fieldnames = [\"Texto\", \"Clasificación\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    for resultado in resultados:\n",
    "        writer.writerow({\"Texto\": resultado[\"Texto\"], \"Clasificación\": resultado[\"Clasificación\"]})\n",
    " \n",
    "    \n",
    "#Falta agregar la columna de clasificación a nuestro data set.\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
